{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fruit_ripeness_title"
      },
      "source": [
        "# üçé Fruit Ripeness Classification Training\n",
        "\n",
        "This notebook trains a deep learning model to classify fruit ripeness using transfer learning with MobileNetV2.\n",
        "\n",
        "## Setup Instructions\n",
        "1. Upload your dataset to Google Drive in this structure:\n",
        "   ```\n",
        "   /content/drive/MyDrive/fruit_dataset/\n",
        "   ‚îú‚îÄ‚îÄ train/\n",
        "   ‚îÇ   ‚îú‚îÄ‚îÄ banana_ripe/\n",
        "   ‚îÇ   ‚îú‚îÄ‚îÄ banana_unripe/\n",
        "   ‚îÇ   ‚îú‚îÄ‚îÄ apple_ripe/\n",
        "   ‚îÇ   ‚îî‚îÄ‚îÄ apple_unripe/\n",
        "   ‚îî‚îÄ‚îÄ val/ (optional)\n",
        "   ```\n",
        "2. Run all cells in order\n",
        "3. Download the trained model and labels.txt\n",
        "4. Place them in your project's `models/` directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_imports"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Imports\n",
        "import os\n",
        "import pathlib\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import zipfile\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config_params"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "DATA_PATH = \"/content/drive/MyDrive/fruit_dataset\"  # Update this path\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS_INITIAL = 10\n",
        "EPOCHS_FINETUNE = 5\n",
        "LEARNING_RATE_INITIAL = 0.001\n",
        "LEARNING_RATE_FINETUNE = 0.0001\n",
        "\n",
        "# Check if data path exists\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    print(f\"‚ùå Data path not found: {DATA_PATH}\")\n",
        "    print(\"Please upload your dataset to Google Drive and update DATA_PATH\")\n",
        "else:\n",
        "    print(f\"‚úÖ Data path found: {DATA_PATH}\")\n",
        "    print(f\"Contents: {os.listdir(DATA_PATH)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_loading"
      },
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "train_dir = os.path.join(DATA_PATH, \"train\")\n",
        "val_dir = os.path.join(DATA_PATH, \"val\")\n",
        "\n",
        "# Check if separate validation directory exists\n",
        "if os.path.exists(val_dir) and os.listdir(val_dir):\n",
        "    print(\"üìÅ Using separate train/val directories\")\n",
        "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        train_dir,\n",
        "        image_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True\n",
        "    )\n",
        "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        val_dir,\n",
        "        image_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False\n",
        "    )\n",
        "else:\n",
        "    print(\"üìÅ Splitting training data (80/20)\")\n",
        "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        train_dir,\n",
        "        image_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        validation_split=0.2,\n",
        "        subset=\"training\",\n",
        "        seed=42\n",
        "    )\n",
        "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        train_dir,\n",
        "        image_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        validation_split=0.2,\n",
        "        subset=\"validation\",\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "num_classes = len(class_names)\n",
        "\n",
        "print(f\"üìä Found {num_classes} classes: {class_names}\")\n",
        "print(f\"üî¢ Training batches: {tf.data.experimental.cardinality(train_ds).numpy()}\")\n",
        "print(f\"üî¢ Validation batches: {tf.data.experimental.cardinality(val_ds).numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_visualization"
      },
      "outputs": [],
      "source": [
        "# Visualize some training images\n",
        "plt.figure(figsize=(12, 8))\n",
        "for images, labels in train_ds.take(1):\n",
        "    for i in range(min(9, len(images))):\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(f\"{class_names[labels[i]]}\")\n",
        "        plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_preparation"
      },
      "outputs": [],
      "source": [
        "# Optimize dataset performance\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.prefetch(AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(AUTOTUNE)\n",
        "\n",
        "# Data augmentation\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.1),\n",
        "    tf.keras.layers.RandomZoom(0.1),\n",
        "    tf.keras.layers.RandomContrast(0.1),\n",
        "])\n",
        "\n",
        "print(\"‚úÖ Data preparation complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_creation"
      },
      "outputs": [],
      "source": [
        "# Create model with transfer learning\n",
        "base_model = tf.keras.applications.MobileNetV2(\n",
        "    input_shape=IMG_SIZE + (3,),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "base_model.trainable = False  # Freeze base model initially\n",
        "\n",
        "# Model architecture\n",
        "inputs = tf.keras.Input(shape=IMG_SIZE + (3,))\n",
        "x = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
        "x = data_augmentation(x)\n",
        "x = base_model(x, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_INITIAL),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"üèóÔ∏è Model created successfully\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "initial_training"
      },
      "outputs": [],
      "source": [
        "# Initial training (frozen base)\n",
        "print(f\"üöÄ Starting initial training ({EPOCHS_INITIAL} epochs)...\")\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=3,\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=2,\n",
        "        min_lr=1e-7\n",
        "    )\n",
        "]\n",
        "\n",
        "history1 = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS_INITIAL,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Initial training complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fine_tuning"
      },
      "outputs": [],
      "source": [
        "# Fine-tuning (unfreeze some layers)\n",
        "print(\"üîß Fine-tuning model...\")\n",
        "base_model.trainable = True\n",
        "\n",
        "# Freeze early layers, fine-tune later layers\n",
        "for layer in base_model.layers[:-20]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Recompile with lower learning rate\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_FINETUNE),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Continue training\n",
        "history2 = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS_FINETUNE,\n",
        "    initial_epoch=len(history1.history['accuracy']),\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Fine-tuning complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot_results"
      },
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "def plot_training_history(hist1, hist2):\n",
        "    # Combine histories\n",
        "    acc = hist1.history['accuracy'] + hist2.history['accuracy']\n",
        "    val_acc = hist1.history['val_accuracy'] + hist2.history['val_accuracy']\n",
        "    loss = hist1.history['loss'] + hist2.history['loss']\n",
        "    val_loss = hist1.history['val_loss'] + hist2.history['val_loss']\n",
        "    \n",
        "    epochs_range = range(len(acc))\n",
        "    \n",
        "    plt.figure(figsize=(12, 4))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "    plt.axvline(x=len(hist1.history['accuracy'])-1, color='r', linestyle='--', alpha=0.5, label='Fine-tuning starts')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Training Loss')\n",
        "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "    plt.axvline(x=len(hist1.history['loss'])-1, color='r', linestyle='--', alpha=0.5, label='Fine-tuning starts')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history1, history2)\n",
        "\n",
        "# Final evaluation\n",
        "val_loss, val_accuracy = model.evaluate(val_ds, verbose=0)\n",
        "print(f\"üìä Final validation accuracy: {val_accuracy:.4f}\")\n",
        "print(f\"üìä Final validation loss: {val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_model"
      },
      "outputs": [],
      "source": [
        "# Save model and create download package\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "model_name = f\"fruit_model_{timestamp}\"\n",
        "\n",
        "# Save model\n",
        "model.save(f\"/content/{model_name}\")\n",
        "\n",
        "# Save labels\n",
        "with open(f\"/content/{model_name}/labels.txt\", 'w') as f:\n",
        "    for class_name in class_names:\n",
        "        f.write(f\"{class_name}\\n\")\n",
        "\n",
        "# Create zip file for download\n",
        "zip_path = f\"/content/{model_name}.zip\"\n",
        "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, dirs, files in os.walk(f\"/content/{model_name}\"):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(file_path, f\"/content/{model_name}\")\n",
        "            zipf.write(file_path, arcname)\n",
        "\n",
        "print(f\"üíæ Model saved as: {model_name}\")\n",
        "print(f\"üì¶ Download package created: {zip_path}\")\n",
        "print(\"\\nüéØ Next steps:\")\n",
        "print(\"1. Download the zip file from the Files panel\")\n",
        "print(\"2. Extract it to your project's models/ directory\")\n",
        "print(\"3. Update MODEL_PATH in your .env file\")\n",
        "print(\"4. Restart your Flask app\")\n",
        "\n",
        "# Show download link\n",
        "from google.colab import files\n",
        "print(\"\\n‚¨áÔ∏è Downloading model...\")\n",
        "files.download(zip_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_predictions"
      },
      "outputs": [],
      "source": [
        "# Test some predictions\n",
        "plt.figure(figsize=(15, 10))\n",
        "for images, labels in val_ds.take(1):\n",
        "    predictions = model.predict(images)\n",
        "    for i in range(min(9, len(images))):\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        \n",
        "        predicted_class = class_names[np.argmax(predictions[i])]\n",
        "        true_class = class_names[labels[i]]\n",
        "        confidence = np.max(predictions[i])\n",
        "        \n",
        "        color = 'green' if predicted_class == true_class else 'red'\n",
        "        plt.title(f\"True: {true_class}\\nPred: {predicted_class} ({confidence:.2f})\", color=color)\n",
        "        plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
